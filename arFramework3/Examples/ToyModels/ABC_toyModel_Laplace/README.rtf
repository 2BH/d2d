{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf830
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red160\green32\blue240;}
{\*\expandedcolortbl;;\csgenericrgb\c62745\c12549\c94118;}
\paperw11900\paperh16840\margl1440\margr1440\vieww15420\viewh12460\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 README:\
\
This example introduces a Laplace distribution for the error model ( {\field{\*\fldinst{HYPERLINK "https://en.wikipedia.org/wiki/Laplace_distribution"}}{\fldrslt https://en.wikipedia.org/wiki/Laplace_distribution}} ). You cannot perform the usual fitting with the .m files in this folder!!\
\
Thus, the likelihood function and its sensitivities are different from the usual Gaussian error model and are calculated in the function arCalcRes.m in this folder.\
Also, -Log likelihood instead of -2log likelihood is used in this case, and the altered objective function is computed in arGetMerit.m\
\
To fit a model to data given Laplace distributed errors, ar.config.optimizer=19 has to be set!\
Then, the corresponding interior-point optimizer of the MATLAB function fmincon is called in arFit.m, with gradient provided and Hessian approximated via 'bfgs\'92. The fitting works with simultaneous error estimation.\
The distribution can be validated via 'check_laplace.m\'92.\
\
\
To apply the Laplace distribution to a different model, include the lines:\
\
\pard\pardeftab720\partightenfactor0

\f1\fs20 \cf0 ar.config.optimizer = 19;
\fs24 \

\fs20 ar.config.optimizers = [ar.config.optimizers \cf2 'fmincon_laplace'\cf0 ];
\fs24 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf0 \
in your Setup file _and_ copy the files\
\
arCalcRes.m\
arFit.m\
arGetMerit.m\
arPLEInit.m\
\
to your model folder.\
\
\
## Warning ##\
\
Fitting with the Laplace error distribution provided here substantially hampers fit convergence, e.g. within a LHS.\
Since it is non-continuous at 0, the optimization steps are likely to fail every time a residual switches its sign (increasingly often for bigger models), causing step control to shrink the step.\
In addition, parameter profiles often become non-identifiable since for example the activation of C in this example has simulated data with finite slope, yet the objective function is still statistically valid for a step-like instantaneous activation, treating the slope as 'outlier'.}